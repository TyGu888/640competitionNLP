{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T21:41:50.975039Z","iopub.status.busy":"2022-11-09T21:41:50.974575Z","iopub.status.idle":"2022-11-09T21:41:57.146102Z","shell.execute_reply":"2022-11-09T21:41:57.145056Z","shell.execute_reply.started":"2022-11-09T21:41:50.975002Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["TF version: 2.10.0\n","transformers version: 4.24.0\n"]},{"name":"stderr","output_type":"stream","text":["f:\\anaconda\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os, gc\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","print(f'TF version: {tf.__version__}')\n","import tensorflow_addons as tfa\n","from tensorflow.keras import layers\n","\n","import transformers\n","print(f'transformers version: {transformers.__version__}')\n","from transformers import logging as hf_logging\n","hf_logging.set_verbosity_error()\n","\n","import sys\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["sys.path.append('./input/iterativestratification')\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T21:42:02.362147Z","iopub.status.busy":"2022-11-09T21:42:02.361535Z","iopub.status.idle":"2022-11-09T21:42:02.369685Z","shell.execute_reply":"2022-11-09T21:42:02.368721Z","shell.execute_reply.started":"2022-11-09T21:42:02.362113Z"},"trusted":true},"outputs":[],"source":["def set_seed(seed=1225):\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","#     os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","set_seed(1225)"]},{"cell_type":"markdown","metadata":{},"source":["# Load DataFrame"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T21:42:07.792565Z","iopub.status.busy":"2022-11-09T21:42:07.792211Z","iopub.status.idle":"2022-11-09T21:42:08.033322Z","shell.execute_reply":"2022-11-09T21:42:08.032415Z","shell.execute_reply.started":"2022-11-09T21:42:07.792535Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_id</th>\n","      <th>full_text</th>\n","      <th>cohesion</th>\n","      <th>syntax</th>\n","      <th>vocabulary</th>\n","      <th>phraseology</th>\n","      <th>grammar</th>\n","      <th>conventions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0016926B079C</td>\n","      <td>I think that students would benefit from learn...</td>\n","      <td>3.5</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0022683E9EA5</td>\n","      <td>When a problem is a change you have to let it ...</td>\n","      <td>2.5</td>\n","      <td>2.5</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00299B378633</td>\n","      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n","      <td>3.0</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>003885A45F42</td>\n","      <td>The best time in life is when you become yours...</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0049B1DF5CCC</td>\n","      <td>Small act of kindness can impact in other peop...</td>\n","      <td>2.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.5</td>\n","      <td>2.5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        text_id                                          full_text  cohesion  \\\n","0  0016926B079C  I think that students would benefit from learn...       3.5   \n","1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n","2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n","3  003885A45F42  The best time in life is when you become yours...       4.5   \n","4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n","\n","   syntax  vocabulary  phraseology  grammar  conventions  \n","0     3.5         3.0          3.0      4.0          3.0  \n","1     2.5         3.0          2.0      2.0          2.5  \n","2     3.5         3.0          3.0      3.0          2.5  \n","3     4.5         4.5          4.5      4.0          5.0  \n","4     3.0         3.0          3.0      2.5          2.5  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","---------DataFrame Summary---------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3911 entries, 0 to 3910\n","Data columns (total 8 columns):\n"," #   Column       Non-Null Count  Dtype  \n","---  ------       --------------  -----  \n"," 0   text_id      3911 non-null   object \n"," 1   full_text    3911 non-null   object \n"," 2   cohesion     3911 non-null   float64\n"," 3   syntax       3911 non-null   float64\n"," 4   vocabulary   3911 non-null   float64\n"," 5   phraseology  3911 non-null   float64\n"," 6   grammar      3911 non-null   float64\n"," 7   conventions  3911 non-null   float64\n","dtypes: float64(6), object(2)\n","memory usage: 244.6+ KB\n"]}],"source":["df = pd.read_csv('./input/feedback-prize-english-language-learning/train.csv')\n","display(df.head())\n","print('\\n---------DataFrame Summary---------')\n","df.info()"]},{"cell_type":"markdown","metadata":{},"source":["# CV Split"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T21:42:14.432739Z","iopub.status.busy":"2022-11-09T21:42:14.432317Z","iopub.status.idle":"2022-11-09T21:42:14.568916Z","shell.execute_reply":"2022-11-09T21:42:14.567639Z","shell.execute_reply.started":"2022-11-09T21:42:14.432684Z"},"trusted":true},"outputs":[{"data":{"text/plain":["1    783\n","0    782\n","4    782\n","3    782\n","2    782\n","Name: fold, dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["N_FOLD = 5\n","TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n","\n","skf = MultilabelStratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=42)\n","for n, (train_index, val_index) in enumerate(skf.split(df, df[TARGET_COLS])):\n","    df.loc[val_index, 'fold'] = int(n)\n","df['fold'] = df['fold'].astype(int)\n","df['fold'].value_counts()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T21:42:18.008426Z","iopub.status.busy":"2022-11-09T21:42:18.008060Z","iopub.status.idle":"2022-11-09T21:42:18.145824Z","shell.execute_reply":"2022-11-09T21:42:18.144916Z","shell.execute_reply.started":"2022-11-09T21:42:18.008396Z"},"trusted":true},"outputs":[],"source":["df.to_csv('./df_folds.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Config"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T21:42:19.737835Z","iopub.status.busy":"2022-11-09T21:42:19.737466Z","iopub.status.idle":"2022-11-09T21:42:19.742964Z","shell.execute_reply":"2022-11-09T21:42:19.741941Z","shell.execute_reply.started":"2022-11-09T21:42:19.737802Z"},"trusted":true},"outputs":[],"source":["MAX_LENGTH = 512\n","BATCH_SIZE = 2\n","DEBERTA_MODEL = \"./input/deberta-v3-base\""]},{"cell_type":"markdown","metadata":{},"source":["Why we should disable dropout in regression task, check this [discussion](https://www.kaggle.com/competitions/commonlitreadabilityprize/discussion/260729)."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T21:42:54.848512Z","iopub.status.busy":"2022-11-09T21:42:54.848145Z","iopub.status.idle":"2022-11-09T21:42:56.537370Z","shell.execute_reply":"2022-11-09T21:42:56.536339Z","shell.execute_reply.started":"2022-11-09T21:42:54.848479Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["f:\\anaconda\\envs\\tf\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]}],"source":["tokenizer = transformers.AutoTokenizer.from_pretrained(DEBERTA_MODEL)\n","tokenizer.save_pretrained('./tokenizer/')\n","\n","cfg = transformers.AutoConfig.from_pretrained(DEBERTA_MODEL, output_hidden_states=True)\n","cfg.hidden_dropout_prob = 0\n","cfg.attention_probs_dropout_prob = 0\n","cfg.save_pretrained('./tokenizer/')"]},{"cell_type":"markdown","metadata":{},"source":["# Data Process Function\n","\n","To make use of HugggingFace DeBERTa model, we have to tokenize our input texts as the pretrained DeBERTa model requires."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T21:43:39.643566Z","iopub.status.busy":"2022-11-09T21:43:39.643206Z","iopub.status.idle":"2022-11-09T21:43:39.650154Z","shell.execute_reply":"2022-11-09T21:43:39.649104Z","shell.execute_reply.started":"2022-11-09T21:43:39.643535Z"},"trusted":true},"outputs":[],"source":["def deberta_encode(texts, tokenizer=tokenizer):\n","    input_ids = []\n","    attention_mask = []\n","    \n","    for text in texts.tolist():\n","        token = tokenizer(text, \n","                          add_special_tokens=True, \n","                          max_length=MAX_LENGTH, \n","                          return_attention_mask=True, \n","                          return_tensors=\"np\", \n","                          truncation=True, \n","                          padding='max_length')\n","        input_ids.append(token['input_ids'][0])\n","        attention_mask.append(token['attention_mask'][0])\n","    \n","    return np.array(input_ids, dtype=\"int32\"), np.array(attention_mask, dtype=\"int32\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T21:43:42.901724Z","iopub.status.busy":"2022-11-09T21:43:42.901351Z","iopub.status.idle":"2022-11-09T21:43:42.907220Z","shell.execute_reply":"2022-11-09T21:43:42.905760Z","shell.execute_reply.started":"2022-11-09T21:43:42.901678Z"},"trusted":true},"outputs":[],"source":["def get_dataset(df):\n","    inputs = deberta_encode(df['full_text'])\n","    targets = np.array(df[TARGET_COLS], dtype=\"float32\")\n","    return inputs, targets"]},{"cell_type":"markdown","metadata":{},"source":["# Model\n","\n","## MeanPool\n","\n","Instead of using '[CLS]' token, MeanPool method averaging one layer of hidden states along the sequence axis with masking out padding tokens.\n","\n","## WeightedLayerPool\n","\n","WeightedLayerPool uses a set of trainable weights to average a set of hidden states from transformer backbone. I use a Dense layer with constraint to implement it."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T21:44:03.564283Z","iopub.status.busy":"2022-11-09T21:44:03.563581Z","iopub.status.idle":"2022-11-09T21:44:03.570545Z","shell.execute_reply":"2022-11-09T21:44:03.569393Z","shell.execute_reply.started":"2022-11-09T21:44:03.564244Z"},"trusted":true},"outputs":[],"source":["class MeanPool(tf.keras.layers.Layer):\n","    def call(self, inputs, mask=None):\n","        broadcast_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n","        embedding_sum = tf.reduce_sum(inputs * broadcast_mask, axis=1)\n","        mask_sum = tf.reduce_sum(broadcast_mask, axis=1)\n","        mask_sum = tf.math.maximum(mask_sum, tf.constant([1e-9]))\n","        return embedding_sum / mask_sum"]},{"cell_type":"markdown","metadata":{},"source":["WeightedLayerPool weights constraints: softmax to push sum(w) to be 1."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T21:44:06.361748Z","iopub.status.busy":"2022-11-09T21:44:06.361362Z","iopub.status.idle":"2022-11-09T21:44:06.366534Z","shell.execute_reply":"2022-11-09T21:44:06.365213Z","shell.execute_reply.started":"2022-11-09T21:44:06.361693Z"},"trusted":true},"outputs":[],"source":["class WeightsSumOne(tf.keras.constraints.Constraint):\n","    def __call__(self, w):\n","        return tf.nn.softmax(w, axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Design \n","\n","Take the last 4 layers hidden states of DeBERTa, take MeanPool of them to gather information along the sequence axis, then take WeightedLayerPool with a set of trainable weights to gather information along the depth axis of the model, then finally a regression head.\n","\n","## Last Layer Reinitialization\n","\n","Reinitialization of the last transformer encoder block: GlorotUniform for Dense kernel, Zeros for Dense bias, Zeros for LayerNorm beta, Ones for LayerNorm gamma.\n","\n","## Layer-wise Learning Rate Decay\n","\n","Using MultiOptimizer to implement LLRD: Initial learning rate 1e-5 with layer-wise decay 0.9 for transformer encoder and embedding block, 1e-4 for the rest of the model. All learning rates have ExponentialDecay schedulers with decay rate 0.3"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T22:01:55.632561Z","iopub.status.busy":"2022-11-09T22:01:55.632193Z","iopub.status.idle":"2022-11-09T22:01:55.652677Z","shell.execute_reply":"2022-11-09T22:01:55.651506Z","shell.execute_reply.started":"2022-11-09T22:01:55.632528Z"},"trusted":true},"outputs":[],"source":["def get_model():\n","    input_ids = tf.keras.layers.Input(\n","        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\"\n","    )\n","    \n","    attention_masks = tf.keras.layers.Input(\n","        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"attention_masks\"\n","    )\n","   \n","    deberta_model = transformers.TFAutoModel.from_pretrained(DEBERTA_MODEL, config=cfg)\n","    \n","    #Last Layer Reinitialization or Partially Reinitialization\n","#     Uncommon next three lines to check deberta encoder block\n","#     print('DeBERTa Encoder Block:')\n","#     for layer in deberta_model.deberta.encoder.layer:\n","#         print(layer)\n","        \n","    REINIT_LAYERS = 1\n","    normal_initializer = tf.keras.initializers.GlorotUniform()\n","    zeros_initializer = tf.keras.initializers.Zeros()\n","    ones_initializer = tf.keras.initializers.Ones()\n","\n","#     print(f'\\nRe-initializing encoder block:')\n","    for encoder_block in deberta_model.deberta.encoder.layer[-REINIT_LAYERS:]:\n","#         print(f'{encoder_block}')\n","        for layer in encoder_block.submodules:\n","            if isinstance(layer, tf.keras.layers.Dense):\n","                layer.kernel.assign(normal_initializer(shape=layer.kernel.shape, dtype=layer.kernel.dtype))\n","                if layer.bias is not None:\n","                    layer.bias.assign(zeros_initializer(shape=layer.bias.shape, dtype=layer.bias.dtype))\n","\n","            elif isinstance(layer, tf.keras.layers.LayerNormalization):\n","                layer.beta.assign(zeros_initializer(shape=layer.beta.shape, dtype=layer.beta.dtype))\n","                layer.gamma.assign(ones_initializer(shape=layer.gamma.shape, dtype=layer.gamma.dtype))\n","\n","    deberta_output = deberta_model.deberta(\n","        input_ids, attention_mask=attention_masks\n","    )\n","    hidden_states = deberta_output.hidden_states\n","    \n","    #WeightedLayerPool + MeanPool of the last 4 hidden states\n","    stack_meanpool = tf.stack(\n","        [MeanPool()(hidden_s, mask=attention_masks) for hidden_s in hidden_states[-4:]], \n","        axis=2)\n","    \n","    weighted_layer_pool = layers.Dense(1,\n","                                       use_bias=False,\n","                                       kernel_constraint=WeightsSumOne())(stack_meanpool)\n","    \n","    weighted_layer_pool = tf.squeeze(weighted_layer_pool, axis=-1)\n","    #x=layers.Dense(4096,activation='relu')(weighted_layer_pool)\n","    output = layers.Dense(6, activation='linear')(weighted_layer_pool)\n","    \n","    #output = layers.Rescaling(scale=4.0, offset=1.0)(x)\n","    model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output)\n","    \n","    #Compile model with Layer-wise Learning Rate Decay\n","    layer_list = [deberta_model.deberta.embeddings] + list(deberta_model.deberta.encoder.layer)\n","    layer_list.reverse()\n","    \n","    INIT_LR = 1e-5\n","    LLRDR = 0.9\n","    LR_SCH_DECAY_STEPS = 1600 # 2 * len(train_df) // BATCH_SIZE\n","    \n","    lr_schedules = [tf.keras.optimizers.schedules.ExponentialDecay(\n","        initial_learning_rate=INIT_LR * LLRDR ** i, \n","        decay_steps=LR_SCH_DECAY_STEPS, \n","        decay_rate=0.3) for i in range(len(layer_list))]\n","    lr_schedule_head = tf.keras.optimizers.schedules.ExponentialDecay(\n","        initial_learning_rate=1e-4, \n","        decay_steps=LR_SCH_DECAY_STEPS, \n","        decay_rate=0.3)\n","    \n","    optimizers = [tf.keras.optimizers.Adam(learning_rate=lr_sch) for lr_sch in lr_schedules]\n","    \n","    optimizers_and_layers = [(tf.keras.optimizers.Adam(learning_rate=lr_schedule_head), model.layers[-4:])] +\\\n","        list(zip(optimizers, layer_list))\n","    \n","#     Uncomment next three lines to check optimizers_and_layers\n","#     print('\\nLayer-wise Learning Rate Decay Initial LR:')\n","#     for o,l in optimizers_and_layers:\n","#         print(f'{o._decayed_lr(\"float32\").numpy()} for {l}')\n","        \n","    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n","    \n","    model.compile(optimizer=optimizer,\n","                 loss='mse',\n","                 metrics=[tf.keras.metrics.RootMeanSquaredError()],\n","                 )\n","    return model"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T22:02:02.491622Z","iopub.status.busy":"2022-11-09T22:02:02.490616Z","iopub.status.idle":"2022-11-09T22:02:14.266934Z","shell.execute_reply":"2022-11-09T22:02:14.265675Z","shell.execute_reply.started":"2022-11-09T22:02:02.491586Z"},"trusted":true},"outputs":[],"source":["tf.keras.backend.clear_session()\n","#model = get_model()\n","#model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["# 5 Folds Training Loop"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-11-09T22:02:34.186998Z","iopub.status.busy":"2022-11-09T22:02:34.186512Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","-----------FOLD 0 ------------\n","Data prepared.\n","Training data input_ids shape: (3129, 512) dtype: int32\n","Training data attention_mask shape: (3129, 512) dtype: int32\n","Training data targets shape: (3129, 6) dtype: float32\n","Validation data input_ids shape: (782, 512) dtype: int32\n","Validation data attention_mask shape: (782, 512) dtype: int32\n","Validation data targets shape: (782, 6) dtype: float32\n"]},{"name":"stderr","output_type":"stream","text":["f:\\anaconda\\envs\\tf\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Model prepared.\n","Start training...\n","Epoch 1/10\n","WARNING:tensorflow:From f:\\anaconda\\envs\\tf\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_tf_deberta_v2.py:125: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n","Instructions for updating:\n","The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n","WARNING:tensorflow:From f:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\bernoulli.py:86: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n","Instructions for updating:\n","The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n","3129/3129 [==============================] - ETA: 0s - loss: 0.2784 - root_mean_squared_error: 0.5277\n","Epoch 1: val_loss improved from inf to 0.20902, saving model to best_model_fold0.h5\n","3129/3129 [==============================] - 297s 87ms/step - loss: 0.2784 - root_mean_squared_error: 0.5277 - val_loss: 0.2090 - val_root_mean_squared_error: 0.4572\n","Epoch 2/10\n","3129/3129 [==============================] - ETA: 0s - loss: 0.1983 - root_mean_squared_error: 0.4453\n","Epoch 2: val_loss improved from 0.20902 to 0.20666, saving model to best_model_fold0.h5\n","3129/3129 [==============================] - 253s 81ms/step - loss: 0.1983 - root_mean_squared_error: 0.4453 - val_loss: 0.2067 - val_root_mean_squared_error: 0.4546\n","Epoch 3/10\n","3129/3129 [==============================] - ETA: 0s - loss: 0.1940 - root_mean_squared_error: 0.4404\n","Epoch 3: val_loss improved from 0.20666 to 0.20625, saving model to best_model_fold0.h5\n","3129/3129 [==============================] - 248s 79ms/step - loss: 0.1940 - root_mean_squared_error: 0.4404 - val_loss: 0.2063 - val_root_mean_squared_error: 0.4541\n","Epoch 4/10\n","3129/3129 [==============================] - ETA: 0s - loss: 0.1934 - root_mean_squared_error: 0.4397"]}],"source":["valid_rmses = []\n","for fold in range(N_FOLD):\n","    print(f'\\n-----------FOLD {fold} ------------')\n","    \n","    #Create dataset\n","    train_df = df[df['fold'] != fold].reset_index(drop=True)\n","    valid_df = df[df['fold'] == fold].reset_index(drop=True)\n","    train_dataset = get_dataset(train_df)\n","    valid_dataset = get_dataset(valid_df)\n","    \n","    print('Data prepared.')\n","    print(f'Training data input_ids shape: {train_dataset[0][0].shape} dtype: {train_dataset[0][0].dtype}') \n","    print(f'Training data attention_mask shape: {train_dataset[0][1].shape} dtype: {train_dataset[0][1].dtype}')\n","    print(f'Training data targets shape: {train_dataset[1].shape} dtype: {train_dataset[1].dtype}')\n","    print(f'Validation data input_ids shape: {valid_dataset[0][0].shape} dtype: {valid_dataset[0][0].dtype}')\n","    print(f'Validation data attention_mask shape: {valid_dataset[0][1].shape} dtype: {valid_dataset[0][1].dtype}')\n","    print(f'Validation data targets shape: {valid_dataset[1].shape} dtype: {valid_dataset[1].dtype}')\n","    \n","    #Create model\n","    tf.keras.backend.clear_session()\n","    model = get_model()\n","    print('Model prepared.')\n","    \n","    #Training model\n","    print('Start training...')\n","    callbacks = [\n","    tf.keras.callbacks.ModelCheckpoint(f\"best_model_fold{fold}.h5\",\n","                                       monitor=\"val_loss\",\n","                                       mode=\"min\",\n","                                       save_best_only=True,\n","                                       verbose=1,\n","                                       save_weights_only=True,),\n","    tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n","                                     min_delta=1e-5, \n","                                     patience=3, \n","                                     verbose=1,\n","                                     mode='min',)\n","    ]\n","    history = model.fit(x=train_dataset[0],\n","                        y=train_dataset[1],\n","                        validation_data=valid_dataset, \n","                        epochs=10,\n","                        shuffle=True,\n","                        batch_size=1,\n","                        callbacks=callbacks\n","                       )\n","    \n","    valid_rmses.append(np.min(history.history['val_root_mean_squared_error']))\n","    print('Training finished.')\n","    del train_dataset, valid_dataset, train_df, valid_df\n","    gc.collect()\n","    tf.keras.backend.clear_session()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f'{len(valid_rmses)} Folds validation RMSE:\\n{valid_rmses}')\n","print(f'Local CV Average score: {np.mean(valid_rmses)}')"]},{"cell_type":"markdown","metadata":{},"source":["# Inference\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n","test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_dataset = deberta_encode(test_df['full_text'])"]},{"cell_type":"markdown","metadata":{},"source":["# Single model 5 Folds ensemble prediction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fold_preds = []\n","for fold in range(N_FOLD):\n","    tf.keras.backend.clear_session()\n","    model = get_model()\n","    model.load_weights(f'best_model_fold{fold}.h5')\n","    print(f'\\nFold {fold} inference...')\n","    pred = model.predict(test_dataset, batch_size=BATCH_SIZE)\n","    fold_preds.append(pred)\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preds = np.mean(fold_preds, axis=0)\n","preds = np.clip(preds, 1, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sub_df = pd.concat([test_df[['text_id']], pd.DataFrame(preds, columns=TARGET_COLS)], axis=1)\n","sub_df.to_csv('submission2.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sub_df.head()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('tf')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"51a272811b0571923bf2d77daa8ff520cc1868d3ddada5cf849f84d1c97129e4"}}},"nbformat":4,"nbformat_minor":4}
