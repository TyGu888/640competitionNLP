{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91fa5cf",
   "metadata": {},
   "source": [
    "# DeBERTa 4-Way Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b3c7e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a04dde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:11:59.533981Z",
     "iopub.status.busy": "2022-11-29T02:11:59.533186Z",
     "iopub.status.idle": "2022-11-29T02:12:06.200451Z",
     "shell.execute_reply": "2022-11-29T02:12:06.199393Z"
    },
    "papermill": {
     "duration": 6.678,
     "end_time": "2022-11-29T02:12:06.202991",
     "exception": false,
     "start_time": "2022-11-29T02:11:59.524991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.6.4\n",
      "transformers version: 4.20.1\n"
     ]
    }
   ],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f'TF version: {tf.__version__}')\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import transformers\n",
    "print(f'transformers version: {transformers.__version__}')\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e082421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:12:06.215889Z",
     "iopub.status.busy": "2022-11-29T02:12:06.215359Z",
     "iopub.status.idle": "2022-11-29T02:12:06.221313Z",
     "shell.execute_reply": "2022-11-29T02:12:06.220330Z"
    },
    "papermill": {
     "duration": 0.014618,
     "end_time": "2022-11-29T02:12:06.223454",
     "exception": false,
     "start_time": "2022-11-29T02:12:06.208836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=25):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d93caa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:12:06.235488Z",
     "iopub.status.busy": "2022-11-29T02:12:06.235188Z",
     "iopub.status.idle": "2022-11-29T02:12:06.240045Z",
     "shell.execute_reply": "2022-11-29T02:12:06.239202Z"
    },
    "papermill": {
     "duration": 0.013247,
     "end_time": "2022-11-29T02:12:06.242001",
     "exception": false,
     "start_time": "2022-11-29T02:12:06.228754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_FOLD = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279f5a1",
   "metadata": {},
   "source": [
    "## Get Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a809da57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:12:06.253705Z",
     "iopub.status.busy": "2022-11-29T02:12:06.253435Z",
     "iopub.status.idle": "2022-11-29T02:12:06.258167Z",
     "shell.execute_reply": "2022-11-29T02:12:06.257269Z"
    },
    "papermill": {
     "duration": 0.01298,
     "end_time": "2022-11-29T02:12:06.260244",
     "exception": false,
     "start_time": "2022-11-29T02:12:06.247264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(df):\n",
    "    inputs = deberta_encode(df['full_text'])\n",
    "    targets = np.array(df[TARGET_COLS], dtype=\"float32\")\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a3095b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:12:06.272018Z",
     "iopub.status.busy": "2022-11-29T02:12:06.271762Z",
     "iopub.status.idle": "2022-11-29T02:12:06.278180Z",
     "shell.execute_reply": "2022-11-29T02:12:06.277326Z"
    },
    "papermill": {
     "duration": 0.014664,
     "end_time": "2022-11-29T02:12:06.280241",
     "exception": false,
     "start_time": "2022-11-29T02:12:06.265577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPool(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, mask=None):\n",
    "        broadcast_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n",
    "        embedding_sum = tf.reduce_sum(inputs * broadcast_mask, axis=1)\n",
    "        mask_sum = tf.reduce_sum(broadcast_mask, axis=1)\n",
    "        mask_sum = tf.math.maximum(mask_sum, tf.constant([1e-9]))\n",
    "        return embedding_sum / mask_sum\n",
    "        \n",
    "class WeightsSumOne(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        return tf.nn.softmax(w, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c414b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:12:06.354743Z",
     "iopub.status.busy": "2022-11-29T02:12:06.354046Z",
     "iopub.status.idle": "2022-11-29T02:12:08.014753Z",
     "shell.execute_reply": "2022-11-29T02:12:08.013760Z"
    },
    "papermill": {
     "duration": 1.669908,
     "end_time": "2022-11-29T02:12:08.017655",
     "exception": false,
     "start_time": "2022-11-29T02:12:06.347747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/debertav3base\")\n",
    "tokenizer.save_pretrained('./tokenizer/')\n",
    "\n",
    "cfg = transformers.AutoConfig.from_pretrained(\"../input/debertav3base\", output_hidden_states=True)\n",
    "cfg.hidden_dropout_prob = 0\n",
    "cfg.attention_probs_dropout_prob = 0\n",
    "cfg.save_pretrained('./tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1284aac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:12:08.030476Z",
     "iopub.status.busy": "2022-11-29T02:12:08.030134Z",
     "iopub.status.idle": "2022-11-29T02:12:08.036569Z",
     "shell.execute_reply": "2022-11-29T02:12:08.035591Z"
    },
    "papermill": {
     "duration": 0.014914,
     "end_time": "2022-11-29T02:12:08.038540",
     "exception": false,
     "start_time": "2022-11-29T02:12:08.023626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deberta_encode(texts, tokenizer=tokenizer):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for text in texts.tolist():\n",
    "        token = tokenizer(text, \n",
    "                          add_special_tokens=True, \n",
    "                          max_length=MAX_LENGTH, \n",
    "                          return_attention_mask=True, \n",
    "                          return_tensors=\"np\", \n",
    "                          truncation=True, \n",
    "                          padding='max_length')\n",
    "        input_ids.append(token['input_ids'][0])\n",
    "        attention_mask.append(token['attention_mask'][0])\n",
    "    \n",
    "    return np.array(input_ids, dtype=\"int32\"), np.array(attention_mask, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0597c8fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:12:08.050458Z",
     "iopub.status.busy": "2022-11-29T02:12:08.050171Z",
     "iopub.status.idle": "2022-11-29T02:12:08.064039Z",
     "shell.execute_reply": "2022-11-29T02:12:08.063187Z"
    },
    "papermill": {
     "duration": 0.022078,
     "end_time": "2022-11-29T02:12:08.066032",
     "exception": false,
     "start_time": "2022-11-29T02:12:08.043954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c7b43c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:12:08.077713Z",
     "iopub.status.busy": "2022-11-29T02:12:08.077456Z",
     "iopub.status.idle": "2022-11-29T02:12:08.099272Z",
     "shell.execute_reply": "2022-11-29T02:12:08.098328Z"
    },
    "papermill": {
     "duration": 0.030063,
     "end_time": "2022-11-29T02:12:08.101395",
     "exception": false,
     "start_time": "2022-11-29T02:12:08.071332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH=512\n",
    "BATCH_SIZE=8\n",
    "test_dataset = deberta_encode(test_df['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade26a2",
   "metadata": {},
   "source": [
    "## v3base-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "258be416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:12:08.113601Z",
     "iopub.status.busy": "2022-11-29T02:12:08.113198Z",
     "iopub.status.idle": "2022-11-29T02:12:08.129230Z",
     "shell.execute_reply": "2022-11-29T02:12:08.128438Z"
    },
    "papermill": {
     "duration": 0.024418,
     "end_time": "2022-11-29T02:12:08.131077",
     "exception": false,
     "start_time": "2022-11-29T02:12:08.106659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_v3_large():\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    \n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "   \n",
    "    deberta_model = transformers.TFAutoModel.from_pretrained(\"../input/debertav3base\", config=cfg)\n",
    "    \n",
    "    #Last Layer Reinitialization or Partially Reinitialization\n",
    "#     Uncommon next three lines to check deberta encoder block\n",
    "#     print('DeBERTa Encoder Block:')\n",
    "#     for layer in deberta_model.deberta.encoder.layer:\n",
    "#         print(layer)\n",
    "        \n",
    "    REINIT_LAYERS = 1\n",
    "    normal_initializer = tf.keras.initializers.GlorotUniform()\n",
    "    zeros_initializer = tf.keras.initializers.Zeros()\n",
    "    ones_initializer = tf.keras.initializers.Ones()\n",
    "\n",
    "#     print(f'\\nRe-initializing encoder block:')\n",
    "    for encoder_block in deberta_model.deberta.encoder.layer[-REINIT_LAYERS:]:\n",
    "#         print(f'{encoder_block}')\n",
    "        for layer in encoder_block.submodules:\n",
    "            if isinstance(layer, tf.keras.layers.Dense):\n",
    "                layer.kernel.assign(normal_initializer(shape=layer.kernel.shape, dtype=layer.kernel.dtype))\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.assign(zeros_initializer(shape=layer.bias.shape, dtype=layer.bias.dtype))\n",
    "\n",
    "            elif isinstance(layer, tf.keras.layers.LayerNormalization):\n",
    "                layer.beta.assign(zeros_initializer(shape=layer.beta.shape, dtype=layer.beta.dtype))\n",
    "                layer.gamma.assign(ones_initializer(shape=layer.gamma.shape, dtype=layer.gamma.dtype))\n",
    "\n",
    "    deberta_output = deberta_model.deberta(\n",
    "        input_ids, attention_mask=attention_masks\n",
    "    )\n",
    "    hidden_states = deberta_output.hidden_states\n",
    "    \n",
    "    #WeightedLayerPool + MeanPool of the last 4 hidden states\n",
    "    stack_meanpool = tf.stack(\n",
    "        [MeanPool()(hidden_s, mask=attention_masks) for hidden_s in hidden_states[-4:]], \n",
    "        axis=2)\n",
    "    \n",
    "    weighted_layer_pool = layers.Dense(\n",
    "        1,\n",
    "        use_bias=False,\n",
    "        kernel_constraint=WeightsSumOne()\n",
    "        )(stack_meanpool)\n",
    "    \n",
    "    weighted_layer_pool = tf.squeeze(weighted_layer_pool, axis=-1)\n",
    "    output=layers.Dense(6,activation='linear')(weighted_layer_pool)\n",
    "    #x = layers.Dense(6, activation='linear')(x)\n",
    "    \n",
    "    #output = layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "    \n",
    "    #Compile model with Layer-wise Learning Rate Decay\n",
    "    layer_list = [deberta_model.deberta.embeddings] + list(deberta_model.deberta.encoder.layer)\n",
    "    layer_list.reverse()\n",
    "    \n",
    "    INIT_LR = 1e-5\n",
    "    LLRDR = 0.9\n",
    "    LR_SCH_DECAY_STEPS = 1600 # 2 * len(train_df) // BATCH_SIZE\n",
    "    \n",
    "    lr_schedules = [tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=INIT_LR * LLRDR ** i, \n",
    "        decay_steps=LR_SCH_DECAY_STEPS, \n",
    "        decay_rate=0.3) for i in range(len(layer_list))]\n",
    "        \n",
    "    lr_schedule_head = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-4, \n",
    "        decay_steps=LR_SCH_DECAY_STEPS, \n",
    "        decay_rate=0.3)\n",
    "    \n",
    "    optimizers = [tf.keras.optimizers.Adam(learning_rate=lr_sch) for lr_sch in lr_schedules]\n",
    "    \n",
    "    optimizers_and_layers = [(tf.keras.optimizers.Adam(learning_rate=lr_schedule_head), model.layers[-4:])] +\\\n",
    "        list(zip(optimizers, layer_list))\n",
    "    \n",
    "#     Uncomment next three lines to check optimizers_and_layers\n",
    "#     print('\\nLayer-wise Learning Rate Decay Initial LR:')\n",
    "#     for o,l in optimizers_and_layers:\n",
    "#         print(f'{o._decayed_lr(\"float32\").numpy()} for {l}')\n",
    "        \n",
    "    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='mse',\n",
    "                 metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                 )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfd73b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:12:08.144220Z",
     "iopub.status.busy": "2022-11-29T02:12:08.142889Z",
     "iopub.status.idle": "2022-11-29T02:14:51.874110Z",
     "shell.execute_reply": "2022-11-29T02:14:51.873145Z"
    },
    "papermill": {
     "duration": 163.74,
     "end_time": "2022-11-29T02:14:51.876597",
     "exception": false,
     "start_time": "2022-11-29T02:12:08.136597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 02:12:09.786426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 02:12:09.787551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 02:12:09.788260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 02:12:09.789119: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-29 02:12:09.789431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 02:12:09.790144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 02:12:09.790815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 02:12:14.734120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 02:12:14.735161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 02:12:14.735864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 02:12:14.736456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15043 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "2022-11-29 02:12:26.587453: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 02:12:45.472292: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 inference...\n",
      "\n",
      "Fold 2 inference...\n",
      "\n",
      "Fold 3 inference...\n",
      "\n",
      "Fold 4 inference...\n"
     ]
    }
   ],
   "source": [
    "fold_preds = []\n",
    "for fold in range(N_FOLD):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_v3_large = get_model_v3_large()\n",
    "    model_v3_large.load_weights(f'../input/v3base1/best_model_fold{fold}.h5')\n",
    "    print(f'\\nFold {fold} inference...')\n",
    "    pred = model_v3_large.predict(test_dataset, batch_size=BATCH_SIZE)\n",
    "    fold_preds.append(pred)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d817eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:14:51.890229Z",
     "iopub.status.busy": "2022-11-29T02:14:51.889675Z",
     "iopub.status.idle": "2022-11-29T02:14:51.902669Z",
     "shell.execute_reply": "2022-11-29T02:14:51.901841Z"
    },
    "papermill": {
     "duration": 0.021937,
     "end_time": "2022-11-29T02:14:51.904665",
     "exception": false,
     "start_time": "2022-11-29T02:14:51.882728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = np.mean(fold_preds, axis=0)\n",
    "preds = np.clip(preds, 1, 5)\n",
    "TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "sub_df = pd.concat([test_df[['text_id']], pd.DataFrame(preds, columns=TARGET_COLS)], axis=1)\n",
    "sub_df.to_csv('submission0.csv', index=False)\n",
    "#0.4553"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cc63ae",
   "metadata": {
    "papermill": {
     "duration": 0.005714,
     "end_time": "2022-11-29T02:14:51.916361",
     "exception": false,
     "start_time": "2022-11-29T02:14:51.910647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## v3base-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "140d85bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:14:51.929733Z",
     "iopub.status.busy": "2022-11-29T02:14:51.929461Z",
     "iopub.status.idle": "2022-11-29T02:17:19.599144Z",
     "shell.execute_reply": "2022-11-29T02:17:19.598125Z"
    },
    "papermill": {
     "duration": 147.679327,
     "end_time": "2022-11-29T02:17:19.601923",
     "exception": false,
     "start_time": "2022-11-29T02:14:51.922596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 inference...\n",
      "\n",
      "Fold 1 inference...\n",
      "\n",
      "Fold 2 inference...\n",
      "\n",
      "Fold 3 inference...\n",
      "\n",
      "Fold 4 inference...\n"
     ]
    }
   ],
   "source": [
    "fold_preds = []\n",
    "for fold in range(N_FOLD):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_v3_large = get_model_v3_large()\n",
    "    model_v3_large.load_weights(f'../input/v3base2/best_model_fold{fold}.h5')\n",
    "    print(f'\\nFold {fold} inference...')\n",
    "    pred = model_v3_large.predict(test_dataset, batch_size=BATCH_SIZE)\n",
    "    fold_preds.append(pred)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3420464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:17:19.616778Z",
     "iopub.status.busy": "2022-11-29T02:17:19.616058Z",
     "iopub.status.idle": "2022-11-29T02:17:19.625434Z",
     "shell.execute_reply": "2022-11-29T02:17:19.624495Z"
    },
    "papermill": {
     "duration": 0.019295,
     "end_time": "2022-11-29T02:17:19.627790",
     "exception": false,
     "start_time": "2022-11-29T02:17:19.608495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = np.mean(fold_preds, axis=0)\n",
    "preds = np.clip(preds, 1, 5)\n",
    "TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "sub_df = pd.concat([test_df[['text_id']], pd.DataFrame(preds, columns=TARGET_COLS)], axis=1)\n",
    "sub_df.to_csv('submission1.csv', index=False)\n",
    "#0.4566"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199439cd",
   "metadata": {
    "papermill": {
     "duration": 0.006046,
     "end_time": "2022-11-29T02:17:19.652356",
     "exception": false,
     "start_time": "2022-11-29T02:17:19.646310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## base3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479c70c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:17:19.665745Z",
     "iopub.status.busy": "2022-11-29T02:17:19.665481Z",
     "iopub.status.idle": "2022-11-29T02:19:47.021896Z",
     "shell.execute_reply": "2022-11-29T02:19:47.020764Z"
    },
    "papermill": {
     "duration": 147.366293,
     "end_time": "2022-11-29T02:19:47.024877",
     "exception": false,
     "start_time": "2022-11-29T02:17:19.658584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 inference...\n",
      "\n",
      "Fold 1 inference...\n",
      "\n",
      "Fold 2 inference...\n",
      "\n",
      "Fold 3 inference...\n",
      "\n",
      "Fold 4 inference...\n"
     ]
    }
   ],
   "source": [
    "fold_preds = []\n",
    "for fold in range(N_FOLD):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_v3_large = get_model_v3_large()\n",
    "    model_v3_large.load_weights(f'../input/v3base3/v3base-3/best_model_fold{fold}.h5')\n",
    "    print(f'\\nFold {fold} inference...')\n",
    "    pred = model_v3_large.predict(test_dataset, batch_size=BATCH_SIZE)\n",
    "    fold_preds.append(pred)\n",
    "    gc.collect()\n",
    "preds = np.mean(fold_preds, axis=0)\n",
    "preds = np.clip(preds, 1, 5)\n",
    "TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "sub_df = pd.concat([test_df[['text_id']], pd.DataFrame(preds, columns=TARGET_COLS)], axis=1)\n",
    "sub_df.to_csv('submission2.csv', index=False)\n",
    "#0.454"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4462a9",
   "metadata": {
    "papermill": {
     "duration": 0.006411,
     "end_time": "2022-11-29T02:19:47.038707",
     "exception": false,
     "start_time": "2022-11-29T02:19:47.032296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## base4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32ac0bab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:19:47.052875Z",
     "iopub.status.busy": "2022-11-29T02:19:47.052571Z",
     "iopub.status.idle": "2022-11-29T02:22:12.537159Z",
     "shell.execute_reply": "2022-11-29T02:22:12.536128Z"
    },
    "papermill": {
     "duration": 145.494817,
     "end_time": "2022-11-29T02:22:12.539940",
     "exception": false,
     "start_time": "2022-11-29T02:19:47.045123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 inference...\n",
      "\n",
      "Fold 1 inference...\n",
      "\n",
      "Fold 2 inference...\n",
      "\n",
      "Fold 3 inference...\n",
      "\n",
      "Fold 4 inference...\n"
     ]
    }
   ],
   "source": [
    "fold_preds = []\n",
    "for fold in range(N_FOLD):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_v3_large = get_model_v3_large()\n",
    "    model_v3_large.load_weights(f'../input/v3base4/v3base-4/best_model_fold{fold}.h5')\n",
    "    print(f'\\nFold {fold} inference...')\n",
    "    pred = model_v3_large.predict(test_dataset, batch_size=BATCH_SIZE)\n",
    "    fold_preds.append(pred)\n",
    "    gc.collect()\n",
    "preds = np.mean(fold_preds, axis=0)\n",
    "preds = np.clip(preds, 1, 5)\n",
    "TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "sub_df = pd.concat([test_df[['text_id']], pd.DataFrame(preds, columns=TARGET_COLS)], axis=1)\n",
    "sub_df.to_csv('submission3.csv', index=False)\n",
    "#0.458"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b2d1c2",
   "metadata": {
    "papermill": {
     "duration": 0.006453,
     "end_time": "2022-11-29T02:22:12.580214",
     "exception": false,
     "start_time": "2022-11-29T02:22:12.573761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9631abbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T02:22:12.594804Z",
     "iopub.status.busy": "2022-11-29T02:22:12.594503Z",
     "iopub.status.idle": "2022-11-29T02:22:12.642408Z",
     "shell.execute_reply": "2022-11-29T02:22:12.641563Z"
    },
    "papermill": {
     "duration": 0.057535,
     "end_time": "2022-11-29T02:22:12.644276",
     "exception": false,
     "start_time": "2022-11-29T02:22:12.586741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>3.060087</td>\n",
       "      <td>2.916615</td>\n",
       "      <td>3.259661</td>\n",
       "      <td>3.132458</td>\n",
       "      <td>2.845059</td>\n",
       "      <td>2.849767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>2.824525</td>\n",
       "      <td>2.615886</td>\n",
       "      <td>2.900809</td>\n",
       "      <td>2.541753</td>\n",
       "      <td>2.292425</td>\n",
       "      <td>2.748376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.827956</td>\n",
       "      <td>3.622893</td>\n",
       "      <td>3.830468</td>\n",
       "      <td>3.764464</td>\n",
       "      <td>3.608222</td>\n",
       "      <td>3.540919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n",
       "0  0000C359D63E  3.060087  2.916615    3.259661     3.132458  2.845059   \n",
       "1  000BAD50D026  2.824525  2.615886    2.900809     2.541753  2.292425   \n",
       "2  00367BB2546B  3.827956  3.622893    3.830468     3.764464  3.608222   \n",
       "\n",
       "   conventions  \n",
       "0     2.849767  \n",
       "1     2.748376  \n",
       "2     3.540919  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n",
    "submission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n",
    "\n",
    "sub1 = pd.read_csv(f'submission0.csv')[TARGET_COLS] * 1.1\n",
    "sub2 = pd.read_csv(f'submission1.csv')[TARGET_COLS] * 0.9\n",
    "sub3 = pd.read_csv(f'submission2.csv')[TARGET_COLS] * 1.5\n",
    "sub4 = pd.read_csv(f'submission3.csv')[TARGET_COLS] * 1.0\n",
    "\n",
    "ens = ((sub1 + sub2+sub3+sub4)\n",
    "       /(1.1+0.9+1.3+0.9))\n",
    "\n",
    "submission[TARGET_COLS] = ens\n",
    "display(submission.head())\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 624.523501,
   "end_time": "2022-11-29T02:22:15.746562",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-29T02:11:51.223061",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
