{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 4.24.0\n"
     ]
    }
   ],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f'TF version: {tf.__version__}')\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import transformers\n",
    "print(f'transformers version: {transformers.__version__}')\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "import sys\n",
    "sys.path.append('./input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=25):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLD = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(df):\n",
    "    inputs = deberta_encode(df['full_text'])\n",
    "    targets = np.array(df[TARGET_COLS], dtype=\"float32\")\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPool(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, mask=None):\n",
    "        broadcast_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n",
    "        embedding_sum = tf.reduce_sum(inputs * broadcast_mask, axis=1)\n",
    "        mask_sum = tf.reduce_sum(broadcast_mask, axis=1)\n",
    "        mask_sum = tf.math.maximum(mask_sum, tf.constant([1e-9]))\n",
    "        return embedding_sum / mask_sum\n",
    "class WeightsSumOne(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        return tf.nn.softmax(w, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3base-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda\\envs\\tf\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"./input/deberta-v3-base\")\n",
    "tokenizer.save_pretrained('./tokenizer/')\n",
    "\n",
    "cfg = transformers.AutoConfig.from_pretrained(\"./input/deberta-v3-base\", output_hidden_states=True)\n",
    "cfg.hidden_dropout_prob = 0\n",
    "cfg.attention_probs_dropout_prob = 0\n",
    "cfg.save_pretrained('./tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deberta_encode(texts, tokenizer=tokenizer):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for text in texts.tolist():\n",
    "        token = tokenizer(text, \n",
    "                          add_special_tokens=True, \n",
    "                          max_length=MAX_LENGTH, \n",
    "                          return_attention_mask=True, \n",
    "                          return_tensors=\"np\", \n",
    "                          truncation=True, \n",
    "                          padding='max_length')\n",
    "        input_ids.append(token['input_ids'][0])\n",
    "        attention_mask.append(token['attention_mask'][0])\n",
    "    \n",
    "    return np.array(input_ids, dtype=\"int32\"), np.array(attention_mask, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./input/feedback-prize-english-language-learning/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=512\n",
    "BATCH_SIZE=8\n",
    "test_dataset = deberta_encode(test_df['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v3_large():\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    \n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "   \n",
    "    deberta_model = transformers.TFAutoModel.from_pretrained(\"./input/deberta-v3-base\", config=cfg)\n",
    "    \n",
    "    #Last Layer Reinitialization or Partially Reinitialization\n",
    "#     Uncommon next three lines to check deberta encoder block\n",
    "#     print('DeBERTa Encoder Block:')\n",
    "#     for layer in deberta_model.deberta.encoder.layer:\n",
    "#         print(layer)\n",
    "        \n",
    "    REINIT_LAYERS = 1\n",
    "    normal_initializer = tf.keras.initializers.GlorotUniform()\n",
    "    zeros_initializer = tf.keras.initializers.Zeros()\n",
    "    ones_initializer = tf.keras.initializers.Ones()\n",
    "\n",
    "#     print(f'\\nRe-initializing encoder block:')\n",
    "    for encoder_block in deberta_model.deberta.encoder.layer[-REINIT_LAYERS:]:\n",
    "#         print(f'{encoder_block}')\n",
    "        for layer in encoder_block.submodules:\n",
    "            if isinstance(layer, tf.keras.layers.Dense):\n",
    "                layer.kernel.assign(normal_initializer(shape=layer.kernel.shape, dtype=layer.kernel.dtype))\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.assign(zeros_initializer(shape=layer.bias.shape, dtype=layer.bias.dtype))\n",
    "\n",
    "            elif isinstance(layer, tf.keras.layers.LayerNormalization):\n",
    "                layer.beta.assign(zeros_initializer(shape=layer.beta.shape, dtype=layer.beta.dtype))\n",
    "                layer.gamma.assign(ones_initializer(shape=layer.gamma.shape, dtype=layer.gamma.dtype))\n",
    "\n",
    "    deberta_output = deberta_model.deberta(\n",
    "        input_ids, attention_mask=attention_masks\n",
    "    )\n",
    "    hidden_states = deberta_output.hidden_states\n",
    "    \n",
    "    #WeightedLayerPool + MeanPool of the last 4 hidden states\n",
    "    stack_meanpool = tf.stack(\n",
    "        [MeanPool()(hidden_s, mask=attention_masks) for hidden_s in hidden_states[-4:]], \n",
    "        axis=2)\n",
    "    \n",
    "    weighted_layer_pool = layers.Dense(1,\n",
    "                                       use_bias=False,\n",
    "                                       kernel_constraint=WeightsSumOne())(stack_meanpool)\n",
    "    \n",
    "    weighted_layer_pool = tf.squeeze(weighted_layer_pool, axis=-1)\n",
    "    output=layers.Dense(6,activation='linear')(weighted_layer_pool)\n",
    "    #x = layers.Dense(6, activation='linear')(x)\n",
    "    \n",
    "    #output = layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "    \n",
    "    #Compile model with Layer-wise Learning Rate Decay\n",
    "    layer_list = [deberta_model.deberta.embeddings] + list(deberta_model.deberta.encoder.layer)\n",
    "    layer_list.reverse()\n",
    "    \n",
    "    INIT_LR = 1e-5\n",
    "    LLRDR = 0.9\n",
    "    LR_SCH_DECAY_STEPS = 1600 # 2 * len(train_df) // BATCH_SIZE\n",
    "    \n",
    "    lr_schedules = [tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=INIT_LR * LLRDR ** i, \n",
    "        decay_steps=LR_SCH_DECAY_STEPS, \n",
    "        decay_rate=0.3) for i in range(len(layer_list))]\n",
    "    lr_schedule_head = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-4, \n",
    "        decay_steps=LR_SCH_DECAY_STEPS, \n",
    "        decay_rate=0.3)\n",
    "    \n",
    "    optimizers = [tf.keras.optimizers.Adam(learning_rate=lr_sch) for lr_sch in lr_schedules]\n",
    "    \n",
    "    optimizers_and_layers = [(tf.keras.optimizers.Adam(learning_rate=lr_schedule_head), model.layers[-4:])] +\\\n",
    "        list(zip(optimizers, layer_list))\n",
    "    \n",
    "#     Uncomment next three lines to check optimizers_and_layers\n",
    "#     print('\\nLayer-wise Learning Rate Decay Initial LR:')\n",
    "#     for o,l in optimizers_and_layers:\n",
    "#         print(f'{o._decayed_lr(\"float32\").numpy()} for {l}')\n",
    "        \n",
    "    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='mse',\n",
    "                 metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                 )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda\\envs\\tf\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 inference...\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "\n",
      "Fold 1 inference...\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "Fold 2 inference...\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "Fold 3 inference...\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "Fold 4 inference...\n",
      "1/1 [==============================] - 5s 5s/step\n"
     ]
    }
   ],
   "source": [
    "fold_preds = []\n",
    "for fold in range(N_FOLD):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_v3_large = get_model_v3_large()\n",
    "    model_v3_large.load_weights(f'./v3base-1/best_model_fold{fold}.h5')\n",
    "    print(f'\\nFold {fold} inference...')\n",
    "    pred = model_v3_large.predict(test_dataset, batch_size=BATCH_SIZE)\n",
    "    fold_preds.append(pred)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(fold_preds, axis=0)\n",
    "preds = np.clip(preds, 1, 5)\n",
    "TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "sub_df = pd.concat([test_df[['text_id']], pd.DataFrame(preds, columns=TARGET_COLS)], axis=1)\n",
    "sub_df.to_csv('submission0.csv', index=False)\n",
    "#0.4553"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v3base-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda\\envs\\tf\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 inference...\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000214A3AB3160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "\n",
      "Fold 1 inference...\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "Fold 2 inference...\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "\n",
      "Fold 3 inference...\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "\n",
      "Fold 4 inference...\n",
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    }
   ],
   "source": [
    "fold_preds = []\n",
    "for fold in range(N_FOLD):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_v3_large = get_model_v3_large()\n",
    "    model_v3_large.load_weights(f'./v3base-2/best_model_fold{fold}.h5')\n",
    "    print(f'\\nFold {fold} inference...')\n",
    "    pred = model_v3_large.predict(test_dataset, batch_size=BATCH_SIZE)\n",
    "    fold_preds.append(pred)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(fold_preds, axis=0)\n",
    "preds = np.clip(preds, 1, 5)\n",
    "TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "sub_df = pd.concat([test_df[['text_id']], pd.DataFrame(preds, columns=TARGET_COLS)], axis=1)\n",
    "sub_df.to_csv('submission1.csv', index=False)\n",
    "#0.4566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./input/feedback-prize-english-language-learning/sample_submission.csv')\n",
    "\n",
    "sub1 = pd.read_csv(f'submission0.csv')[TARGET_COLS] * 1.1\n",
    "sub2 = pd.read_csv(f'submission1.csv')[TARGET_COLS] * 1.0\n",
    "#sub3 = pd.read_csv(f'submission_2.csv')[CFG3.target_cols] * CFG3.weight\n",
    "#sub4 = pd.read_csv(f'submission_3.csv')[CFG4.target_cols] * CFG4.weight\n",
    "#sub5 = pd.read_csv(f'submission_4.csv')[CFG5.target_cols] * CFG5.weight\n",
    "#sub6 = pd.read_csv(f'submission_5.csv')[CFG6.target_cols] * CFG6.weight\n",
    "#sub7 = pd.read_csv(f'submission_7.csv')[CFG7.target_cols] * CFG7.weight\n",
    "##sub8 = pd.read_csv(f'submission_8.csv')[CFG8.target_cols] * CFG8.weight\n",
    "#sub9 = pd.read_csv(f'submission_9.csv')[CFG9.target_cols] * CFG9.weight\n",
    "#sub10 = pd.read_csv(f'submission_10.csv')[CFG10.target_cols] * CFG10.weight\n",
    "\n",
    "ens = ((sub1 + sub2)\n",
    "       /(1.1+1.0))\n",
    "\n",
    "#ens = (sub1 + sub2)/(CFG1.weight + CFG2.weight)\n",
    "\n",
    "submission[TARGET_COLS] = ens\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>2.833304</td>\n",
       "      <td>2.703496</td>\n",
       "      <td>3.023050</td>\n",
       "      <td>2.910030</td>\n",
       "      <td>2.645960</td>\n",
       "      <td>2.646942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>2.597240</td>\n",
       "      <td>2.443963</td>\n",
       "      <td>2.693636</td>\n",
       "      <td>2.341450</td>\n",
       "      <td>2.138072</td>\n",
       "      <td>2.560819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.546981</td>\n",
       "      <td>3.354390</td>\n",
       "      <td>3.570885</td>\n",
       "      <td>3.499786</td>\n",
       "      <td>3.357464</td>\n",
       "      <td>3.287691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n",
       "0  0000C359D63E  2.833304  2.703496    3.023050     2.910030  2.645960   \n",
       "1  000BAD50D026  2.597240  2.443963    2.693636     2.341450  2.138072   \n",
       "2  00367BB2546B  3.546981  3.354390    3.570885     3.499786  3.357464   \n",
       "\n",
       "   conventions  \n",
       "0     2.646942  \n",
       "1     2.560819  \n",
       "2     3.287691  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51a272811b0571923bf2d77daa8ff520cc1868d3ddada5cf849f84d1c97129e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
